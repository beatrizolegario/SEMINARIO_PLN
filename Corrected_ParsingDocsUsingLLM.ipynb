{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beatrizolegario/SEMINARIO_PLN/blob/main/Corrected_ParsingDocsUsingLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**SEMINÁRIO:** Extração de Dados de Documentos Usando LLMs\n",
        "_Artigo Original_: [Document Parsing Using Large Language Models — With Code](https://towardsdatascience.com/document-parsing-using-large-language-models-with-code-9229fda09cdf)\n",
        "\n",
        "_Notebook Original_: [Extract_Metadata_With_Large_Language_Models](https://github.com/keitazoumana/LLMs/blob/main/Extract_Metadata_With_Large_Language_Models_V2.ipynb)\n",
        "\n",
        "\n",
        "- _Autor: Zoumana Keita_\n",
        "- Revisão dos Dicentes:\n",
        "  - Beatriz Santos Olegario\n",
        "  - Bruno JM de Camargo\n",
        "  -Thiago Bibiano Silva\n",
        "  \n",
        "\n",
        "\n",
        "A extração de dados de documentos complexos, como visto no artigo científico abaixo, teve como única alternativa por algum tempo o uso de expressões regulares. Hoje, com os modelos de linguagem de grande porte, surge uma nova perspectiva para essa tarefa. No artigo de Zoumana Keita, são discutidas as vantagens dos LLMs em relação às expressões regulares, com uma implementação utilizando o modelo GPT-4 da OpenAI.\n",
        "\n",
        "\n",
        "\n",
        "![Descrição da Imagem](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*glQYLm0bwalGhBgppAbbSg.png)\n",
        "\n",
        "\n",
        "_Imagem 1: Exemplo de entidadades para serem extraídas de um artigo_\n",
        "\n",
        "\n",
        "## Comparativo entre Regex e LLM\n",
        "\n",
        "### Expressões Regulares (Regex)\n",
        "**Vantagens:**\n",
        "- Rápido para padrões simples.\n",
        "- Eficiente com estruturas de documentos bem definidas.\n",
        "\n",
        "**Desvantagens:**\n",
        "- Baixa flexibilidade; requer padrões específicos para cada estrutura de documento.\n",
        "- Não compreende o contexto ou significado do texto.\n",
        "- Difícil manutenção; exige atualizações constantes para novos formatos.\n",
        "- Dificuldade em lidar com documentos complexos e variáveis.\n",
        "- Requer conhecimento técnico para criar padrões eficazes.\n",
        "\n",
        "### Modelos de Linguagem de Grande Porte (LLMs)\n",
        "**Vantagens:**\n",
        "- Alta flexibilidade; adapta-se a diversas estruturas de documentos.\n",
        "- Compreensão do significado do texto, permitindo melhor extração de dados.\n",
        "- Fácil manutenção; adapta-se a novos tipos de documentos com mínimas alterações.\n",
        "- Eficaz em lidar com complexidade e variabilidade.\n",
        "- Mais intuitivo, utilizando prompts descritivos.\n",
        "\n",
        "**Desvantagens:**\n",
        "- Pode ser mais lento devido à complexidade do modelo.\n",
        "- Requer recursos computacionais robustos.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "2GvPaxyMZ564"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementação\n",
        "\n",
        "![Descrição da Imagem](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ZZR9KMsYHkZdbdoXe-e3bQ.png)\n",
        "\n",
        "_Imagem 2: Proposta de fluxo de extração de dados propostoto por Keita_\n",
        "\n",
        "Os principais pontos da implementação proposta por Keita são:\n",
        "- Um **extrator de textos** à partir de PDF de artigos científicos com uso de OCR para leitura de imagens\n",
        "- Um **prompt de extração de metadados**, executado usando GPT-4\n",
        "- Estutura de arquivos abaixo:\n",
        "```\n",
        "project\n",
        "   |\n",
        "   |---Extract_Metadata_With_Large_Language_Models.ipynb\n",
        "   |\n",
        "  data\n",
        "   |\n",
        "   |---- extracted_metadata/\n",
        "   |---- 1706.03762v7.pdf\n",
        "   |---- prompts\n",
        "           |\n",
        "           |------ scientific_papers_prompt.txt\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "**Modificações:**\n",
        "\n",
        "- **Extrator de Textos**: Não conseguimos implementar a função original com OCR, então:\n",
        "  - Simplificamos para apenas extrair textos sem apoio de OCR com o uso da lib `PyPDF2`\n",
        "  - Limitamos a coleta para apenas a primeira página, já que isso bastaria para o caso de uso proposto (Coleta de Autores e Resumo)\n",
        "- **Extrator de Metadados**: Nâo modificamos o prompt, mas como a procura estava apenas em texto, resolvemos utilizar  o GPT-40-mini para reduzir custos.\n",
        "  - No exemplo abaixo uma execução com GPT-4o teve custo aproximado de 0.2 Dolar (US) enquanto para a versão mini o custo foi de >0.01 Dolar (US)\n",
        "- **Estutura de Arquivos**: Emulamos a estrutura de pastas e subida de arquivos no código, criando as pastas quando necessário.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2-CHvlIIZ8zO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalndo bibliotecas necessárias\n",
        "\n",
        "Bibliotecas comentadas estavam no artigo original ou não foram usadas devido à simplificação do extrator de texto ou já estavam ociosas no notebook original."
      ],
      "metadata": {
        "id": "Qa9oIGWyHp18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "# Instala bibliotecas necessárias\n",
        "pip install pdfminer.six\n",
        "#pip install pillow-heif==0.3.2\n",
        "#pip install matplotlib\n",
        "#pip install unstructured-inference\n",
        "#pip install unstructured-pytesseract\n",
        "#pip install tesseract-ocr\n",
        "#pip install unstructured\n",
        "#pip install pi-heif\n",
        "pip install openai\n",
        "pip install PyPDF2\n",
        "\n",
        "\n",
        "# Instala pacotes de OCR e utilitários do sistema necessários\n",
        "#apt install -V tesseract-ocr\n",
        "#apt install -V libtesseract-dev\n",
        "\n",
        "#sudo apt-get update\n",
        "#apt-get install -V poppler-utils\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ow4J_FZcZ5ZA",
        "outputId": "fcaf76ce-be61-4e97-bee2-734887467b46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (20231228)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (43.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.43.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classe ResearchPaperProcessor\n",
        "\n",
        "Para facilitar a analise do código e modificações necessárias consolidamos as funções presentes no notebook e outras criadas para facilitar a abstração da classe.\n",
        "\n",
        "\n",
        "1. **`__init__(self, model_id: str)`**\n",
        "   - Inicialização:\n",
        "     - Define `model_id` e configura o cliente OpenAI.\n",
        "\n",
        "2. **`get_client(self)`**\n",
        "   - Configuração de API:\n",
        "     - Obtém a chave da API do Google Colab.\n",
        "     - Configura o cliente OpenAI com a chave.\n",
        "\n",
        "3. **`read_prompt(prompt_path: str) -> str`**\n",
        "   - Leitura de Prompt:\n",
        "     - Abre o arquivo de texto na localização `prompt_path`.\n",
        "     - Retorna o conteúdo como uma string.\n",
        "\n",
        "4. **`extract_text_from_pdf(pdf_path: str) -> str`**\n",
        "   - Extração de Texto:\n",
        "     - Abre o PDF em `pdf_path`.\n",
        "     - Utiliza `PyPDF2` para ler o texto da primeira página.\n",
        "     - Retorna o texto extraído.\n",
        "\n",
        "5. **`completion_with_backoff(self, **kwargs)`**\n",
        "   - Chamada à API com Backoff:\n",
        "     - Faz uma chamada à API OpenAI.\n",
        "     - Utiliza backoff exponencial para lidar com limites de taxa.\n",
        "\n",
        "6. **`extract_metadata(self, content: str, prompt_path: str) -> dict`**\n",
        "   - Preparação:\n",
        "     - Lê o prompt de `prompt_path`.\n",
        "   - Chamada GPT:\n",
        "     - Realiza a chamada à API para extração de metadados.\n",
        "   - Processamento de Resposta:\n",
        "     - Limpa e formata a resposta JSON.\n",
        "     - Corrige erros de análise de JSON, se necessário.\n",
        "\n",
        "7. **`process_research_paper(self, pdf_path: str, prompt_path: str, output_folder: str)`**\n",
        "   - Extração de Texto:\n",
        "     - Extrai o texto do PDF.\n",
        "   - Extração de Metadados:\n",
        "     - Chama `extract_metadata` para obter metadados.\n",
        "   - Salvamento de Resultados:\n",
        "     - Salva os metadados extraídos em um arquivo JSON no `output_folder`."
      ],
      "metadata": {
        "id": "atMOqiNIHvd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importação das bibliotecas\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import openai\n",
        "from pathlib import Path\n",
        "from PyPDF2 import PdfReader\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        "\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Importação de bibliotecas não usadas no código adpatado\n",
        " #from unstructured.partition.pdf import partition_pdf\n",
        "\n",
        "\n",
        "\n",
        "class ResearchPaperProcessor:\n",
        "    \"\"\"\n",
        "    Classe para processar artigos de pesquisa em PDF, extrair texto e metadados usando a API OpenAI.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_id: str):\n",
        "        self.model_id = model_id\n",
        "        self.client = self.get_client()\n",
        "\n",
        "    @staticmethod\n",
        "    def get_client(self):\n",
        "        \"\"\"\n",
        "        Retorna o cliente OpenAI.]\n",
        "        \"\"\"\n",
        "        OPENAI_API_KEY = userdata.get('OPEN_AI_KEY')\n",
        "        os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "        return OpenAI(api_key = os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "    @staticmethod\n",
        "    def read_prompt(prompt_path: str) -> str:\n",
        "        \"\"\"\n",
        "        Lê o prompt para análise de artigos de pesquisa de um arquivo de texto.\n",
        "        \"\"\"\n",
        "        with open(prompt_path, \"r\") as f:\n",
        "            return f.read()\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "        \"\"\"\n",
        "        Extrai texto de um arquivo PDF.\n",
        "        \"\"\"\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            reader = PdfReader(file)\n",
        "            text = \"\"\n",
        "            page = reader.pages[0]\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "        return text\n",
        "\n",
        "    @retry(wait=wait_random_exponential(min=1, max=120), stop=stop_after_attempt(10))\n",
        "    def completion_with_backoff(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Realiza uma chamada de conclusão com a API OpenAI usando backoff exponencial.\n",
        "        \"\"\"\n",
        "        return client.chat.completions.create(**kwargs)\n",
        "\n",
        "    def extract_metadata(self, content: str, prompt_path: str) -> dict:\n",
        "        \"\"\"\n",
        "        Usa o modelo GPT para extrair metadados do conteúdo do artigo de pesquisa com base no prompt fornecido.\n",
        "        \"\"\"\n",
        "        prompt_data = self.read_prompt(prompt_path)\n",
        "\n",
        "        try:\n",
        "            response = self.completion_with_backoff(\n",
        "                model=self.model_id,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": prompt_data},\n",
        "                    {\"role\": \"user\", \"content\": content}\n",
        "                ],\n",
        "                temperature=0.2,\n",
        "            )\n",
        "\n",
        "            response_content = response.choices[0].message.content\n",
        "            if not response_content:\n",
        "                print(\"Resposta vazia do modelo\")\n",
        "                return {}\n",
        "\n",
        "            # Remove os indicadores de blocos de código markdown\n",
        "            response_content = re.sub(r'```json\\s*', '', response_content)\n",
        "            response_content = re.sub(r'\\s*```', '', response_content)\n",
        "\n",
        "            # Tenta fazer parsing do JSON\n",
        "            try:\n",
        "                return json.loads(response_content)\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Falha ao analisar o JSON: {e}\")\n",
        "                print(f\"Resposta bruta: {response_content}\")\n",
        "\n",
        "                # Tenta extrair JSON da resposta\n",
        "                match = re.search(r'\\{.*\\}', response_content, re.DOTALL)\n",
        "                if match:\n",
        "                    try:\n",
        "                        return json.loads(match.group(0))\n",
        "                    except json.JSONDecodeError as jde:\n",
        "                        print(f\"Falha ao extrair JSON válido da resposta: {jde}\")\n",
        "\n",
        "                return {}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao chamar a API OpenAI: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def process_research_paper(self, pdf_path: str, prompt_path: str, output_folder: str):\n",
        "        \"\"\"\n",
        "        Processa um único artigo de pesquisa através de todo o pipeline.\n",
        "        \"\"\"\n",
        "        print(f\"Processando artigo de pesquisa: {pdf_path}\")\n",
        "\n",
        "        try:\n",
        "            # Passo 1: Extrair o conteúdo do texto do PDF\n",
        "            content = self.extract_text_from_pdf(pdf_path)\n",
        "            print(f\"Conteúdo do texto extraído do PDF: {pdf_path}\")\n",
        "\n",
        "            # Passo 2: Extrair metadados usando o modelo GPT\n",
        "            metadata = self.extract_metadata(content, prompt_path)\n",
        "            if not metadata:\n",
        "                print(f\"Falha ao extrair metadados para {pdf_path}\")\n",
        "                return\n",
        "            print(f\"Metadados extraídos usando {self.model_id} para {pdf_path}\")\n",
        "\n",
        "            # Passo 3: Salvar o resultado como um arquivo JSON\n",
        "            output_filename = Path(pdf_path).stem + '.json'\n",
        "            output_path = os.path.join(output_folder, output_filename)\n",
        "\n",
        "            with open(output_path, 'w') as f:\n",
        "                json.dump(metadata, f, indent=2)\n",
        "            print(f\"Metadados salvos em {output_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao processar {pdf_path}: {e}\")"
      ],
      "metadata": {
        "id": "jeDGWyKXe4FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estrutura do Prompt para Extração de Metadados\n",
        "\n",
        "1. **Espaço Reservado para o Documento:**\n",
        "   - `{document}` indica onde o texto completo do documento será inserido para análise.\n",
        "\n",
        "2. **Atribuição de Papel:**\n",
        "   - Define o modelo como um \"especialista em análise de artigos científicos\" para focar na tarefa.\n",
        "\n",
        "3. **Instruções de Extração:**\n",
        "   - Lista as propriedades a serem extraídas (título, ano, autores, etc.) com detalhes específicos.\n",
        "\n",
        "4. **Definição de Atributos:**\n",
        "   - Especifica o formato e detalhes de cada atributo (e.g., \"Author Contact\" como lista de dicionários).\n",
        "\n",
        "5. **Diretrizes:**\n",
        "   - Regras para a extração (precisão, concisão, manejo de dados ausentes).\n",
        "\n",
        "6. **Formato de Saída:**\n",
        "   - Especifica que a resposta deve ser em formato JSON com chaves específicas para cada atributo."
      ],
      "metadata": {
        "id": "eMIZTAzoDeYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_text = '''\n",
        "Scientific research paper:\n",
        "---\n",
        "{document}\n",
        "---\n",
        "\n",
        "You are an expert in analyzing scientific research papers. Please carefully read the provided research paper above and extract the following key information:\n",
        "\n",
        "Extract these six (6) properties from the research paper:\n",
        "- Paper Title: The full title of the research paper\n",
        "- Publication Year: The year the paper was published\n",
        "- Authors: The full names of all authors of the paper\n",
        "- Author Contact: A list of dictionaries, where each dictionary contains the following keys for each author:\n",
        "  - Name: The full name of the author\n",
        "  - Institution: The institutional affiliation of the author\n",
        "  - Email: The email address of the author (if provided)\n",
        "- Abstract: The full text of the paper's abstract\n",
        "- Summary Abstract: A concise summary of the abstract in 2-3 sentences, highlighting the key points\n",
        "\n",
        "Guidelines:\n",
        "- The extracted information should be factual and accurate to the document.\n",
        "- Be extremely concise, except for the Abstract which should be copied in full.\n",
        "- The extracted entities should be self-contained and easily understood without the rest of the paper.\n",
        "- If any property is missing from the paper, please leave the field empty rather than guessing.\n",
        "- For the Summary Abstract, focus on the main objectives, methods, and key findings of the research.\n",
        "- For Author Contact, create an entry for each author, even if some information is missing. If an email or institution is not provided for an author, leave that field empty in the dictionary.\n",
        "\n",
        "Answer in JSON format. The JSON should contain 6 keys: \"PaperTitle\", \"PublicationYear\", \"Authors\", \"AuthorContact\", \"Abstract\", and \"SummaryAbstract\". The \"AuthorContact\" should be a list of dictionaries as described above.\n",
        "'''\n",
        "\n",
        "prompt_path = \"./data/prompts/scientific_papers_prompt.txt\"\n",
        "\n",
        "os.makedirs(\"./data/prompts/\", exist_ok=True)\n",
        "with open(prompt_path, \"w\") as f:\n",
        "    f.write(prompt_text)"
      ],
      "metadata": {
        "id": "ubhnTzeQclXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exemplo de Uso\n",
        "\n",
        "### Baixando artigo \"Attention Is All You Need\""
      ],
      "metadata": {
        "id": "9MKsdGL-EVDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# URL do PDF para download\n",
        "url = \"https://arxiv.org/pdf/1706.03762v7\"\n",
        "\n",
        "# Caminho para salvar o PDF baixado\n",
        "pdf_path = \"./data/1706.03762v7.pdf\"\n",
        "\n",
        "# Cria o diretório se ele não existir\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "# Faz o download do PDF\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verifica se a requisição foi bem-sucedida\n",
        "if response.status_code == 200:\n",
        "    # Salva o PDF no caminho especificado\n",
        "    with open(pdf_path, \"wb\") as pdf_file:\n",
        "        pdf_file.write(response.content)\n",
        "    print(f\"PDF baixado e salvo em {pdf_path}\")\n",
        "else:\n",
        "    print(f\"Falha ao baixar o PDF. Código de status: {response.status_code}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPFrjB_3c3LL",
        "outputId": "b960506c-826c-454b-de58-ceaae8e09892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF baixado e salvo em ./data/1706.03762v7.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chamada da Classe ResearchPaperProcessor"
      ],
      "metadata": {
        "id": "4EVKkujMFqkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Inicializa o processador de artigos com o modelo desejado\n",
        "processor = ResearchPaperProcessor(model_id='gpt-4o-mini')\n",
        "\n",
        "# Define os caminhos para o PDF, prompt e pasta de saída\n",
        "pdf_path = \"./data/1706.03762v7.pdf\"\n",
        "prompt_path =  \"./data/prompts/scientific_papers_prompt.txt\"\n",
        "output_folder = \"./data/extracted_metadata\"\n",
        "\n",
        "# Cria a pasta de saída se ela não existir\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Processa o artigo de pesquisa\n",
        "processor.process_research_paper(pdf_path, prompt_path, output_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzuErwgXejsh",
        "outputId": "b148c746-639e-4128-ce54-64d308e6ab7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processando artigo de pesquisa: ./data/1706.03762v7.pdf\n",
            "Conteúdo do texto extraído do PDF: ./data/1706.03762v7.pdf\n",
            "Metadados extraídos usando gpt-4o-mini para ./data/1706.03762v7.pdf\n",
            "Metadados salvos em ./data/extracted_metadata/1706.03762v7.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizando JSON Resultante"
      ],
      "metadata": {
        "id": "PfbHsCTDF1ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Define the path to the JSON file\n",
        "file_path = './data/extracted_metadata/1706.03762v7.json'\n",
        "\n",
        "# Read the JSON file\n",
        "try:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "        # Output the content of the JSON file\n",
        "        print(json.dumps(data, indent=4))  # Pretty print the JSON data\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file at {file_path} was not found.\")\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"Error decoding JSON: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZiOsOh5enGO",
        "outputId": "e67630f6-631d-45f8-f327-857ef8d83f56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"PaperTitle\": \"Attention Is All You Need\",\n",
            "    \"PublicationYear\": \"2017\",\n",
            "    \"Authors\": [\n",
            "        \"Ashish Vaswani\",\n",
            "        \"Noam Shazeer\",\n",
            "        \"Niki Parmar\",\n",
            "        \"Jakob Uszkoreit\",\n",
            "        \"Llion Jones\",\n",
            "        \"Aidan N. Gomez\",\n",
            "        \"\\u0141ukasz Kaiser\",\n",
            "        \"Illia Polosukhin\"\n",
            "    ],\n",
            "    \"AuthorContact\": [\n",
            "        {\n",
            "            \"Name\": \"Ashish Vaswani\",\n",
            "            \"Institution\": \"Google Brain\",\n",
            "            \"Email\": \"avaswani@google.com\"\n",
            "        },\n",
            "        {\n",
            "            \"Name\": \"Noam Shazeer\",\n",
            "            \"Institution\": \"Google Brain\",\n",
            "            \"Email\": \"noam@google.com\"\n",
            "        },\n",
            "        {\n",
            "            \"Name\": \"Niki Parmar\",\n",
            "            \"Institution\": \"Google Research\",\n",
            "            \"Email\": \"nikip@google.com\"\n",
            "        },\n",
            "        {\n",
            "            \"Name\": \"Jakob Uszkoreit\",\n",
            "            \"Institution\": \"Google Research\",\n",
            "            \"Email\": \"usz@google.com\"\n",
            "        },\n",
            "        {\n",
            "            \"Name\": \"Llion Jones\",\n",
            "            \"Institution\": \"Google Research\",\n",
            "            \"Email\": \"llion@google.com\"\n",
            "        },\n",
            "        {\n",
            "            \"Name\": \"Aidan N. Gomez\",\n",
            "            \"Institution\": \"University of Toronto\",\n",
            "            \"Email\": \"aidan@cs.toronto.edu\"\n",
            "        },\n",
            "        {\n",
            "            \"Name\": \"\\u0141ukasz Kaiser\",\n",
            "            \"Institution\": \"Google Brain\",\n",
            "            \"Email\": \"lukaszkaiser@google.com\"\n",
            "        },\n",
            "        {\n",
            "            \"Name\": \"Illia Polosukhin\",\n",
            "            \"Institution\": \"\",\n",
            "            \"Email\": \"illia.polosukhin@gmail.com\"\n",
            "        }\n",
            "    ],\n",
            "    \"Abstract\": \"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\",\n",
            "    \"SummaryAbstract\": \"This paper introduces the Transformer, a novel network architecture that relies solely on attention mechanisms, eliminating the need for recurrence and convolutions. The Transformer demonstrates superior performance in machine translation tasks, achieving state-of-the-art BLEU scores while being more efficient in training time.\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[](./data/extracted_metadata/json_output.png)"
      ],
      "metadata": {
        "id": "OTO_V-fnPg6W"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}